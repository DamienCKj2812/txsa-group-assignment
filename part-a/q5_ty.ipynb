{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1ddba4",
   "metadata": {},
   "source": [
    "# Q5. Alternative Approach Implementation (Individual Component)\n",
    "\n",
    "This notebook demonstrates an alternative tokenization technique using **Scikit-learn’s CountVectorizer** and compares it with the group's approach (NLTK, Split, and Regex)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af5ec30",
   "metadata": {},
   "source": [
    "---\n",
    "- ### *Cheah Thong Yau (TP070955)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6240ef1",
   "metadata": {},
   "source": [
    "1️. Implement Tokenization Using an Alternative Approach (3 marks)\n",
    "\n",
    "The `CountVectorizer` from Scikit-learn automatically performs tokenization as part of its preprocessing pipeline. It:\n",
    "\n",
    "- Converts text to lowercase\n",
    "\n",
    "- Removes punctuation\n",
    "\n",
    "- Splits text into word tokens\n",
    "\n",
    "- Builds a vocabulary of unique terms\n",
    "\n",
    "Instead of returning a simple list of words, it generates a feature vocabulary that maps words to numerical representations for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4e56aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Classification is the task of choosing the correct class label for a given input. In basic\n",
      "classification tasks, each input is considered in isolation from all other inputs, and the set of labels is defined in advance. The basic classification task has a number of interesting variants. For example, in multiclass classification, each instance may be assigned multiple labels; in open-class classification, the set of labels is not defined in advance; and in sequence classification, a list of inputs are jointly classified.\n",
      "\n",
      "First 20 tokens:\n",
      "['advance' 'all' 'and' 'are' 'assigned' 'basic' 'be' 'choosing' 'class'\n",
      " 'classification' 'classified' 'considered' 'correct' 'defined' 'each'\n",
      " 'example' 'for' 'from' 'given' 'has']\n",
      "\n",
      "Total unique tokens found: 45\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "#!pip install scikit-learn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the data\n",
    "file_path = '../assets/dataset-a/Data_1.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read().strip()\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the text\n",
    "X = vectorizer.fit_transform([text])\n",
    "\n",
    "# Extract tokens (feature names)\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"\\nFirst 20 tokens:\")\n",
    "print(tokens[:20])\n",
    "\n",
    "print(f\"\\nTotal unique tokens found: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbf3719",
   "metadata": {},
   "source": [
    "2. **Compare and Contrast with the Group’s Approach**\n",
    "\n",
    "| Feature              | NLTK `word_tokenize`                 | Split                                                 | Regex                                     | `CountVectorizer`                                  |\n",
    "| -------------------- | ------------------------------------ | ----------------------------------------------------- | ----------------------------------------- | ------------------------------------------------ |\n",
    "| Output Type          | List of word tokens                  | List of word tokens                                   | List of word tokens                       | Array of unique vocabulary tokens                |\n",
    "| Punctuation          | Retained as separate tokens          | Retained or removed depending on how split is applied | Can retain or remove depending on pattern | Removed automatically                            |\n",
    "| Case Handling        | Preserved unless manually lowercased | Preserved unless manually lowercased                  | Preserved unless manually lowercased      | Automatically lowercased                         |\n",
    "| Tokenization Control | Standard tokenization rules          | Full manual control                                   | Full manual/custom control                | Limited control; tokenization rules are internal |\n",
    "| Primary Purpose      | Linguistic tokenization              | Basic manual splitting                                | Custom pattern-based tokenization         | Machine learning feature extraction              |\n",
    "| Output Format        | Human-readable tokens                | Human-readable tokens                                 | Human-readable tokens                     | Numerical feature-ready representation           |\n",
    "\n",
    "\n",
    "Contrast\n",
    "- NLTK provides standard linguistic tokenization.\n",
    "- Split is the simplest method and gives full manual control but cannot handle punctuation well automatically.\n",
    "- Regex allows custom rules and flexible tokenization for special patterns.\n",
    "- CountVectorizer automatically removes punctuation, lowercases text, and produces a numerical vocabulary suitable for machine learning pipelines.\n",
    "\n",
    "Main difference: group methods focus on linguistic flexibility, while CountVectorizer focuses on numerical preprocessing for ML models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba37ab1",
   "metadata": {},
   "source": [
    "3️. **Evaluation: Better, Worse, or Just Different? (5 Marks)**\n",
    "\n",
    "Why It Is Different\n",
    "\n",
    "`CountVectorizer` is designed for machine learning workflows rather than pure linguistic analysis. It integrates tokenization with preprocessing and numerical encoding in a single step.\n",
    "\n",
    "Pros\n",
    "- Automatically handles preprocessing such as lowercasing and punctuation removal.\n",
    "- Produces numerical features directly usable in classification models.\n",
    "- Allows customization through parameters like:\n",
    "    - max_features (Limit max number of unique words, keeping only the most frequent to reduce dimensionality)\n",
    "    - stop_words   (Removes common words that carry little meaning (e.g., the, is, and, in))\n",
    "    - ngram_range  (Defie range of word combination to extract, capturing word sequences)\n",
    "- Efficient for large-scale datasets.\n",
    "\n",
    "Cons\n",
    "- Removes punctuation and sentence boundaries, structural information such as sentence endings or question marks will be lost.\n",
    "- No context awareness, treating every word independently, making it unsuitable for detailed linguistic analysis.\n",
    "- Tokenization process is less transparent compared to regex or NLTK, all happens internally within the library\n",
    "- Vocabulary size can become large, If not limited using `max_features`, it will greatly increase memory usage and computational cost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
